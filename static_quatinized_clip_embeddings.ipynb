{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2815053",
   "metadata": {},
   "source": [
    "# 모델 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2a661c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6da8edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input name: pixel_values\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "# ONNX 모델 로드 (경로 수정 필요)\n",
    "session = ort.InferenceSession(\"static_model.onnx\", providers=[\"CPUExecutionProvider\"])\n",
    "\n",
    "# 입력 이름 확인 (모델마다 다를 수 있음)\n",
    "input_name = session.get_inputs()[0].name\n",
    "print(\"Input name:\", input_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef287ee1",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/code/ivanpan/pytorch-clip-onnx-to-speed-up-inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fd3b28",
   "metadata": {},
   "source": [
    "## 이미지 전처리 함수 (uint8 유지, 정규화 x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8d1b7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "def preprocess_image(path):\n",
    "    img = Image.open(path).convert(\"RGB\").resize((224, 224))\n",
    "    img_np = np.array(img).astype(np.float32) / 255.0          # [0, 1]\n",
    "    img_np = (img_np - 0.5) / 0.5                               # [-1, 1]\n",
    "    img_np = np.transpose(img_np, (2, 0, 1))                    # (HWC) → (CHW)\n",
    "    img_np = np.expand_dims(img_np, axis=0)                    # (1, C, H, W)\n",
    "    return img_np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9632471e",
   "metadata": {},
   "source": [
    "## CLIP 임베딩 추출 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd740420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. CLIP 임베딩 추출 함수\n",
    "def get_clip_embedding(img_tensor):\n",
    "    output = session.run(None, {input_name: img_tensor})\n",
    "    return output[0].squeeze()  # shape: (512,) or (1024,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb90a4c",
   "metadata": {},
   "source": [
    "## 이미지 데이터 셋 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f8b09d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 작업 디렉토리: c:\\Users\\hnn07\\Documents\\clip_embeddings\n",
      "존재 여부: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"현재 작업 디렉토리:\", os.getcwd())\n",
    "print(\"존재 여부:\", os.path.exists(\"sample_image_set\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8743a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "IMAGE_DIR = \"sample_image_set/\"\n",
    "\n",
    "image_paths = []\n",
    "labels = []\n",
    "\n",
    "for class_dir in os.listdir(IMAGE_DIR):\n",
    "    class_path = os.path.join(IMAGE_DIR, class_dir)\n",
    "    if not os.path.isdir(class_path):\n",
    "        continue\n",
    "\n",
    "    for img_path in glob.glob(os.path.join(class_path, \"*.*\")):\n",
    "        image_paths.append(img_path)\n",
    "        labels.append(class_dir)  # 폴더 이름이 곧 클래스 이름"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f69861",
   "metadata": {},
   "source": [
    "## 임베딩 추출 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e24894e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clip_embedding(img_tensor):\n",
    "    output = session.run(None, {input_name: img_tensor})\n",
    "    return output[0].squeeze()  # (512,) 또는 (1024,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0bf916",
   "metadata": {},
   "source": [
    "## 이미지별 임베딩 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c80eb3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 이미지별 임베딩 추출\n",
    "embeddings = []\n",
    "valid_labels = []\n",
    "\n",
    "\n",
    "for path, label in zip(image_paths, labels):\n",
    "    try:\n",
    "        img_tensor = preprocess_image(path)\n",
    "        emb = get_clip_embedding(img_tensor)\n",
    "        embeddings.append(emb)\n",
    "        valid_labels.append(label)\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] {path}: {e}\")\n",
    "\n",
    "embeddings = np.array(embeddings)\n",
    "valid_labels = np.array(valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3253cdfa",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmanifold\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TSNE\n\u001b[0;32m      3\u001b[0m tsne \u001b[38;5;241m=\u001b[39m TSNE(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, perplexity\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "reduced = tsne.fit_transform(embeddings)\n",
    "\n",
    "# 시각화\n",
    "plt.figure(figsize=(12, 8))\n",
    "unique_labels = list(set(valid_labels))\n",
    "color_map = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "colors = [color_map[lbl] for lbl in valid_labels]\n",
    "\n",
    "scatter = plt.scatter(reduced[:, 0], reduced[:, 1], c=colors, cmap='tab10', s=40)\n",
    "plt.title(\"t-SNE of CLIP Image Embeddings (float32 model)\", fontsize=14)\n",
    "plt.colorbar(scatter, ticks=range(len(unique_labels)), label=\"Class\")\n",
    "plt.grid(True)\n",
    "\n",
    "# 범례 표시\n",
    "handles = [plt.Line2D([], [], marker='o', linestyle='', label=lbl,\n",
    "                      color=plt.cm.tab10(color_map[lbl]/10.0)) for lbl in unique_labels]\n",
    "plt.legend(handles=handles, title=\"Class\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clip-embeddings",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
